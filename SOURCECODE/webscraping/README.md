# Research
'https://docs.google.com/document/d/1OkcPKFcgNrbBAVMRlJ71zjSEtTVP-cdGRAhO6y-EPeQ/edit'

Requirements
install them using:
pip install -r requirements.txt
    requests
    w3lib
    html-text
    jstyleson
    scrapy
    scrapy-deltafetch
    extruct
    mysql.connector

# Web Scraper
Currently only implemented for simplyrecipes.com and allrecipes.com

# Running scraper by itself
Run `py __init__.py` in the simply_scraper/spiders/ folder to run scraper for simplyrecipes.com.

Run `py __init__.py` in the allrecipes_scraper/spiders/ folder to run scraper for allrecipes.com.

Scraped info is stored in simplyrecipes.json or allrecipes.json as a Python dict file in simply_scraper or allrecipes_scraper folders

# Running json_to_sql.py by itself
Requires a json file named simplyrecipe.json or allrecipes.json (created by the scrapers)

Requires password change to work with different MySQL connection, and requires that rechefdb is created. 

Run `py json_to_sql.py` in the simply_scraper folder to run and place the contents of the json into the database.

# Running scraper and json_to_sql all at once with run_python.java
Run `java run_python` in the simply_scraper or allrecipes_scraper folder to run 
and generate a json file of scraped recipes then place the contents of the json into the database.

items.py, middlewares.py, pipelines.py, as well as the .scrapy folder and pycache folders
are all required to make scrapy run / get generated by scrapy running


# queryTesting
holds experiments and explanations of queries from user inputted ingredients.

# Testing folder
Contains initial attempts at webscraping used mostly for experimenting. 
